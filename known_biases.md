Data is always [biased](http://blogs.hbr.org/2013/04/the-hidden-biases-in-big-data/), both at the collection and analytical stages. Some of those present in this dataset are pretty obvious when you look at the code (which I've tried hard to document well). These biases can never be eliminated, but they can be foregrounded, so here are some of the imperfections and permutations in this dataset that I know about: 

* It doesn't include any of [Becca's](http://mitadmissions.org/blogs/profile/rheywood) blog posts, since for some reason some of hers are broken and thus break the script.
* The entry text extraction uses the [HTML2Story](http://www.datasciencetoolkit.org/developerdocs#html2story) endpoint of the [DataScienceToolkit](http://www.datasciencetoolkit.org/)API and sometimes includes/excludes other text on the page (i.e., an author's profile). It also can't pull text out of pictures for bloggers who posted hand-drawn comics, meaning (e.g.) the word count is a lower-bound.
* The blogs predate some of the analytical engines now used to measure engagement. We didn't install Google Analytics on the blogs until August 2009, so traffic from the first five years is not included. It's also not clear to me how far back Facebook and Twitter's APIs go (in terms of counting, e.g., shared links), but either way, it's not as far back as the history of the blogs!
* The entity recognition and extraction was done with [CLIFF](http://cliff.mediameter.org), and has some pretty predictable/regular issues, such as assuming that Cambridge (MA) means Cambridge (UK) in a lot of blog posts. 